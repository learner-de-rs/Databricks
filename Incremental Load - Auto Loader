#Auto Loader code for csv files ingestion
//More Efficient
//Can process millions of files


from pyspark.sql.functions import col, current_timestamp
csv_folder_path='dbfs:/FileStore/Auto_loader_files/*'
table_name=f'auto_loader'
checkpoint_path = f"dbfs:/user/hive/warehouse/"

# Clear out data from previous demo execution
spark.sql(f"DROP TABLE IF EXISTS {table_name}")
dbutils.fs.rm(checkpoint_path, True)

(spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "csv")
  .option("cloudFiles.schemaLocation", checkpoint_path)
  .load(csv_folder_path)
  .writeStream
  .option("checkpointLocation", checkpoint_path)
  .trigger(availableNow=True)
  .toTable(table_name))
